\documentclass{article}

\usepackage[final]{neurips_2024}
\usepackage[utf8]{inputenc}
\usepackage[hyphens]{url} 
\usepackage{hyperref}
\hypersetup{breaklinks=true}


\title{Generative Models and Transformer-Based Approaches on Multiple Modalities \\
\large (Group Haha)}

\author{
  Hoang Phuc Trong \\
  FPT University \\
  \texttt{tronghpse183203@fpt.edu.vn} \\
  \And
  Le Quang That \\
  FPT University \\
  \texttt{thatlq183256@fpt.edu.vn} \\
}

\begin{document}

\maketitle

\begin{abstract}
Recent advances in deep learning, particularly in generative modeling and transformer-based architectures, have led to breakthroughs across multiple data modalities. This project explores the application of generative and transformer-based models to a range of challenging tasks including music source separation, video object matting, protein structure prediction, and multimodal few-shot classification. We aim to understand the generalization capabilities of these models when adapted to different types of data—text, images, audio, and biological sequences—using publicly available datasets and pretrained models.
\end{abstract}

\section{Introduction}

Transformer-based models and generative architectures have reshaped AI research. Originally developed for NLP, these models now excel in vision, audio, biology, and multimodal tasks. Generative models like GANs and diffusion models achieve impressive synthesis results, while transformers offer generalization across domains. This project investigates their use in tasks such as music separation, video matting, protein folding, and multimodal classification.

\section{Datasets and Tasks}

We work on four tasks, each with dedicated datasets:

\begin{itemize}
    \item \textbf{Music Separation:} MUSDB18~\cite{musdb18}
    \item \textbf{Video Matting:} YouTube-VOS~\cite{xu2018youtubevos}
    \item \textbf{Protein Structure Generation:} ProteinNet~\cite{alquraishi2019prot}
    \item \textbf{Multimodal Few-Shot Classification:} LAION-400M~\cite{schuhmann2021laion400m}
\end{itemize}

These tasks span multiple modalities with varied challenges such as noisy data, long sequences, and weak supervision.

\section{Related Work}

Recent models demonstrate domain-specific and cross-domain success:

\begin{itemize}
    \item \textbf{Demucs}~\cite{defossez2021demucs}: audio source separation with convolutional transformers.
    \item \textbf{MODNet}~\cite{ke2020modnet}: video matting using semantic priors and attention.
    \item \textbf{AlphaFold2}~\cite{jumper2021alphafold}: highly accurate protein structure prediction using transformers.
    \item \textbf{CLIP and Flamingo}~\cite{radford2021clip,alayrac2022flamingo}: few-shot visual-language understanding.
\end{itemize}

These works motivate our exploration of model transferability across modalities.

\section{Proposed Methodology}

For each task, we will:

\begin{itemize}
    \item Run or fine-tune existing pretrained models (e.g., Demucs, MODNet, OpenFold, CLIP).
    \item Compare training results with inference-only pipelines.
    \item Analyze latent representations and attention maps.
\end{itemize}

We use PyTorch, HuggingFace, and compatible data loaders. Evaluation is both quantitative and qualitative.

\section{Evaluation Strategy}

Evaluation metrics per task:

\begin{itemize}
    \item \textbf{Music:} Signal-to-Distortion Ratio (SDR)
    \item \textbf{Video:} F-measure, Mean Absolute Difference
    \item \textbf{Protein:} RMSD, GDT
    \item \textbf{Multimodal:} Top-1 Accuracy, Zero-shot Precision
\end{itemize}

Learning curves and error visualization will help assess overfitting or transfer quality.

\section{Expected Outcomes}

We aim to demonstrate that generative and transformer-based models generalize across domains with minimal modification. We expect insights into architectural limits and the effect of pretraining, with practical takeaways for future multimodal applications.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
